{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPR Automation ( Additional Checks for mischivious data from sites)\n",
    "\n",
    "### What's New (key features)- last update on 27-10-2023: \n",
    "1. It reports the charged values. \n",
    "2. It checks for the negative value. \n",
    "3. This version reverts back the stage in case of failure of code run\n",
    "4. It saves Data Bank stages for the last 7 days. \n",
    "5. This version now includes the vendor that is common in both Steel and MDPE sheet.\n",
    "6. Bug fixed in \"drop duplicates for final_df\" --> Line 318\n",
    "7. Automatically send the Data Bank and site DPR for reports (progress of 15th and last day of month)\n",
    "8. Sends \"Threatning mails to TE persons, in case of non receipt of DPR timely (1 pm daily)\n",
    "9. Counts total number of reminders sent to each GA and maintain the delay track date wise\n",
    "10. Sends the mail in Hindi with hindi numeral dates\n",
    "11. Now it checks for Abnormally high value of laying/Welding/Conversion/Riser/Meter (update 13.09.2023)\n",
    "12. It generates warning messages for cases where: charging>laying>welding and also for conversion>meter>riser (update 13.09.2023)\n",
    "13. It now warns about any revision in DPR (provided that the name of the file remains the same) --> If this breaks, revert to the older update from mail. \n",
    "14. Speaks up the warning portion (under trial)\n",
    "\n",
    "\n",
    "### [use the 3.0.1.7[stable- no reminders / 3.0.1.8 [stable- no count of reminders] in case it breaks: Mail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "__author__ = 'Nishant Sinha'                                       # \n",
    "__version__ = '3.0.1.13'                                           #\n",
    "__maintainer__ = 'Nishant Sinha'                                   #\n",
    "__status__ = 'Daily Progerss Report Automation'                    #\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the path to where all files are present:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Documents\\Python Scripts\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\hp\\Documents\\Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from styleframe import StyleFrame\n",
    "from datetime import datetime, timedelta\n",
    "import win32com.client\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from time import sleep,time,ctime,strftime\n",
    "column_settings=[]\n",
    "import pickle\n",
    "from colorama import Fore, Style, Back\n",
    "import imaplib\n",
    "import base64\n",
    "import email\n",
    "import smtplib\n",
    "import shutil\n",
    "import subprocess\n",
    "import win32con\n",
    "import win32gui\n",
    "from collections import Counter\n",
    "import pyttsx3\n",
    "\n",
    "from tkinter import *\n",
    "import tkinter.messagebox\n",
    "\n",
    "from email.message import EmailMessage\n",
    "from email import encoders\n",
    "\n",
    "from PIL import ImageGrab\n",
    "import psutil\n",
    "\n",
    "from getpass import getpass\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "# DATE_IN_DPR= input(\"Enter the string mentioned in today's progress column in DPR obtained: \")\n",
    "DATE_IN_DPR= 'today'\n",
    "\n",
    "recipients= ['nishant@energyworld.biz']\n",
    "cc= ['nishantsinha.repl@outlook.com']\n",
    "\n",
    "send_for_reporting= ['lokeshchaudhary.repl@outlook.com']  #send Data Bank and site DPRs with progress date of 15th and last day of month.\n",
    "mail_dict= {\"test_mail\":['nishantjin05@gmail.com','nishantsinha0502@gmail.com'],\n",
    "\"mail_varanasi\": ['anand.tiwaridatt@gmail.com','ambuj@energyworld.biz','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz'],\n",
    "\"mail_bhubaneshwar\": ['virukumar584@gmail.com','niranjan@energyworld.biz','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz'],\n",
    "\"mail_patna\": ['bhakhilesh.er@gmail.com','santosh@energyworld.biz','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz'],\n",
    "\"mail_ranchi\": ['satyaprakash.singh496@gmail.com','abhijeet@energyworld.biz','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz'],\n",
    "\"mail_jamshedpur\": ['akash.repl@outlook.com','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz'],\n",
    "\"mail_cuttack\": ['sibruta@gmail.com','sharvan@energyworld.biz','atulpathak.repl@outlook.com','lokeshchaudhary.repl@outlook.com','nitin@energyworld.biz']}\n",
    "\n",
    "# mail_dict= {\"test_mail\":['nishant.sinha@tractebel.engie.com','nishantjin05@gmail.com','nishantsinha0502@gmail.com']}\n",
    "\n",
    "vd_count,bd_count,pd_count,rd_count,jd_count,cd_count=0,0,0,0,0,0\n",
    "\n",
    "pickle_dpr_reminder_count= open(\"dpr_reminder_count.pickle\",\"rb\")\n",
    "vd=pickle.load(pickle_dpr_reminder_count)\n",
    "vd_rem_count,bd_rem_count,pd_rem_count,rd_rem_count,jd_rem_count,cd_rem_count=vd.iloc[0,0],vd.iloc[0,1],vd.iloc[0,2],vd.iloc[0,3],vd.iloc[0,4],vd.iloc[0,5]\n",
    "pickle_dpr_reminder_count.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If DPR reminder count is messed up somehow, run me to reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dpr_reminder_count=pd.DataFrame(data={'varanasi':[0],'bhubaneshwar':[0],'patna':[0],'ranchi':[0],'jamshedpur':[0],'cuttack':[0]})\n",
    "# pickle_dpr_reminder_count= open(\"dpr_reminder_count.pickle\",\"wb\")\n",
    "# pickle.dump(dpr_reminder_count,pickle_dpr_reminder_count)\n",
    "# pickle_dpr_reminder_count.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Backup of files in workplace (this will have previous day's data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Trial GUI BASED INPUT PROMPT\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# from tkinter import Tk, Label, Button\n",
    "\n",
    "# def on_button_click():\n",
    "#     try:\n",
    "#         os.mkdir(\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "#     except FileExistsError:\n",
    "#         pass\n",
    "#     shutil.rmtree(\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "#     shutil.copytree(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\",\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "#     label.config(text=\"Backup created successfully\")\n",
    "\n",
    "# root = Tk()\n",
    "# root.title(\"Backup Python Script Folder\")\n",
    "\n",
    "# label = Label(root, text=\"Do you want to take a backup of 'Python Script folder?\")\n",
    "# label.pack()\n",
    "\n",
    "# button = Button(root, text=\"Yes\", command=on_button_click)\n",
    "# button.pack()\n",
    "\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to take a backup of 'Python Script folder? Press y/Y to continue'y\n"
     ]
    }
   ],
   "source": [
    "#### Previous state of Python Script folder (irrespective of user_input)\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "shutil.rmtree(\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\")\n",
    "shutil.copytree(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\",\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\")\n",
    "\n",
    "#### Conscious decision to take backup of the previous stage of Python Script folder\n",
    "\n",
    "user_input= input(\"Do you want to take a backup of 'Python Script folder? Press y/Y to continue'\")\n",
    "if user_input in ['y','Y']:\n",
    "    try:\n",
    "        os.mkdir(\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    shutil.rmtree(\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "    shutil.copytree(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\",\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booster Cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to boost up the DPR creation process? Press y/Y to boost, any other key to ignore.\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "## Booster cells\n",
    "boost_up= input(\"Do you want to boost up the DPR creation process? Press y/Y to boost, any other key to ignore.\\n\")\n",
    "if boost_up in ['y','Y']:\n",
    "    os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Pictures\\\\booster_for_dpr.xlsx\"') # included to boost up the process\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_file_name_has= ['BHUBANESHWAR','CUTTACK','JAMSHEDPUR','PATNA','RANCHI','VARANASI']\n",
    "old_folder_name_has= ['COMBINED','OVERSIMPLIFIED','SIMPLIFIED']\n",
    "for j in old_file_name_has:\n",
    "    for i in os.listdir():\n",
    "        try:\n",
    "            if i.lower().split('.')[1] !='pickle':\n",
    "                if (i.lower().split('_')[0] in j.lower()):\n",
    "                    os.remove(i)\n",
    "        except IndexError:\n",
    "            continue\n",
    "try:\n",
    "    for j in old_folder_name_has:\n",
    "        for i in os.listdir():\n",
    "            if i.lower().split('_')[0] in j.lower():\n",
    "                shutil.rmtree(os.getcwd()+'\\\\'+i)\n",
    "except:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from the mail and place it in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m \u001b[1m Check the \"Python Scripts\" folder Nishant! \u001b[39m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # email_user = input('Enter your email ID: ')\n",
    "# email_user= 'gailcgdwork@gmail.com'\n",
    "# # email_pass = getpass('Enter the password: ')\n",
    "# email_pass= '<YOUR PASSWORD HERE>'\n",
    "\n",
    "# mail = imaplib.IMAP4_SSL('imap.gmail.com',993)\n",
    "# mail.login(email_user, email_pass)\n",
    "\n",
    "# mail.select('INBOX')\n",
    "# typ, data = mail.search(None, 'ALL')\n",
    "# mail_ids = data[0]\n",
    "# id_list = mail_ids.split()\n",
    "\n",
    "# for num in data[0].split():\n",
    "#     typ, data = mail.fetch(num, '(RFC822)' )\n",
    "#     raw_email = data[0][1]\n",
    "#     raw_email_string = raw_email.decode('utf-8')\n",
    "#     email_message = email.message_from_string(raw_email_string)\n",
    "#     for part in email_message.walk():\n",
    "        \n",
    "#         if part.get_content_maintype() == 'multipart':\n",
    "#             continue\n",
    "#         if part.get('Content-Disposition') is None:\n",
    "#             continue\n",
    "#         fileName = part.get_filename()\n",
    "#         if bool('.xlsm' in fileName):\n",
    "            \n",
    "#             filePath = os.path.join(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\\", fileName)\n",
    "#             if not os.path.isfile(filePath):\n",
    "#                 fp = open(filePath, 'wb')\n",
    "#                 fp.write(part.get_payload(decode=True))\n",
    "#                 fp.close()\n",
    "#             subject = str(email_message).split(\"Subject: \", 1)[1].split(\"\\nTo:\", 1)[0]\n",
    "#     mail.store(num,'+FLAGS','\\\\Deleted')\n",
    "# mail.expunge()\n",
    "# mail.close()\n",
    "# mail.logout()\n",
    "# print(f'{Fore.CYAN} {Style.BRIGHT} Check the \"Python Scripts\" folder Nishant! {Fore.RESET} {Style.RESET_ALL}')\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "#***************** If the code below breaks, switch to the earlier version commented above ****************\n",
    "###########################################################################################################\n",
    "\n",
    "list_of_files_downloaded=[]\n",
    "# email_user = input('Enter your email ID: ')\n",
    "email_user= 'gailcgdwork@gmail.com'\n",
    "# email_pass = getpass('Enter the password: ')\n",
    "email_pass= '<YOUR PASSWORD HERE>'\n",
    "\n",
    "mail = imaplib.IMAP4_SSL('imap.gmail.com',993)\n",
    "mail.login(email_user, email_pass)\n",
    "\n",
    "mail.select('INBOX')\n",
    "typ, data = mail.search(None, 'ALL')\n",
    "mail_ids = data[0]\n",
    "id_list = mail_ids.split()\n",
    "\n",
    "for num in data[0].split():\n",
    "    typ, data = mail.fetch(num, '(RFC822)' )\n",
    "    raw_email = data[0][1]\n",
    "    raw_email_string = raw_email.decode('utf-8')\n",
    "    email_message = email.message_from_string(raw_email_string)\n",
    "    for part in email_message.walk():\n",
    "        \n",
    "        if part.get_content_maintype() == 'multipart':\n",
    "            continue\n",
    "        if part.get('Content-Disposition') is None:\n",
    "            continue\n",
    "        fileName = part.get_filename()\n",
    "        fileName = fileName.replace(\" \", \"_\").replace(\"-\", \"_\") #added to avoid issues with blank spaces and \".\"\n",
    "        if bool('.xlsm' in fileName):\n",
    "            list_of_files_downloaded.append(fileName)\n",
    "            filePath = os.path.join(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\\", fileName)  \n",
    "            if not os.path.isfile(filePath):\n",
    "                fp = open(filePath, 'wb')\n",
    "                fp.write(part.get_payload(decode=True))\n",
    "                fp.close()\n",
    "            subject = str(email_message).split(\"Subject: \", 1)[1].split(\"\\nTo:\", 1)[0]\n",
    "    mail.store(num,'+FLAGS','\\\\Deleted')\n",
    "mail.expunge()\n",
    "mail.close()\n",
    "mail.logout()\n",
    "\n",
    "print(f'{Fore.CYAN} {Style.BRIGHT} Check the \"Python Scripts\" folder Nishant! {Fore.RESET} {Style.RESET_ALL}')\n",
    "\n",
    "d =  Counter(list_of_files_downloaded)  \n",
    "duplicacy_check = [k for k, v in d.items() if v > 1]\n",
    "\n",
    "if len(duplicacy_check)>0:\n",
    "    root = Tk()\n",
    "    tkinter.messagebox.showinfo(title= \"Duplicacy Alert!!!\", message=f\"Check and Delete the duplicate DPR from {duplicacy_check}\")\n",
    "    root.destroy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Construction DPR New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m \u001b[1m BHUBANESHWAR_DPR_P_06.01.2024____.xlsm found \u001b[39m \u001b[0m\n",
      "\u001b[35m \u001b[1m Cuttack_DPR__06.01.2024.xlsm found \u001b[39m \u001b[0m\n",
      "\u001b[35m \u001b[1m Jamshedpur_DPR_06_01_2024.xlsm found \u001b[39m \u001b[0m\n",
      "\u001b[35m \u001b[1m Patna_DPR_1105_P_06.01.2024__.xlsm found \u001b[39m \u001b[0m\n",
      "\u001b[35m \u001b[1m Ranchi_DPR_06.01.2024.xlsm found \u001b[39m \u001b[0m\n",
      "\u001b[35m \u001b[1m Varanasi_DPR_06.01.2024.xlsm found \u001b[39m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mand_files = ['CGD', 'DF', 'DPR']\n",
    "run_only_if_all_files_are_present = 0\n",
    "new_files = []\n",
    "\n",
    "for j in old_file_name_has:\n",
    "    for i in os.listdir():\n",
    "        if i.lower().split('_')[0] in j.lower() and i.lower().split('_')[0] not in mand_files:\n",
    "            if i.lower().split('.')[1] != 'pickle':\n",
    "                print(f'{Fore.MAGENTA} {Style.BRIGHT} {i} found {Fore.RESET} {Style.RESET_ALL}')\n",
    "                if \"varanasi\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_1 = i\n",
    "                    with open(i, 'r') as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Varanasi_DPR.xlsm\")\n",
    "                if \"bhubaneshwar\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_2 = i\n",
    "                    with open(i, \"r\") as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Bhubaneshwar_DPR.xlsm\")\n",
    "                if \"patna\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_3 = i\n",
    "                    with open(i, \"r\") as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Patna_DPR.xlsm\")\n",
    "                if \"ranchi\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_4 = i\n",
    "                    with open(i, \"r\") as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Ranchi_DPR.xlsm\")\n",
    "                if \"jamshedpur\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_5 = i\n",
    "                    with open(i, \"r\") as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Jamshedpur_DPR.xlsm\")\n",
    "                if \"cuttack\".lower() in i.lower().split('.')[0]:\n",
    "                    remember_me_6 = i\n",
    "                    with open(i, \"r\") as file_handle:\n",
    "                        pass\n",
    "                    os.rename(i, \"Cuttack_DPR.xlsm\")\n",
    "                new_files.append(i)\n",
    "\n",
    "new_file_list = []\n",
    "for i in new_files:\n",
    "    new_file_list.append(i.split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all the files from all the sites have been received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if (not pd.DataFrame(new_file_list).duplicated().any()) & (pd.DataFrame(new_file_list).count()==6)[0]:\n",
    "        run_only_if_all_files_are_present=1\n",
    "except IndexError:\n",
    "    print(f\"{Fore.BLACK}{Back.RED}No site DPR file present in the folder 'Python Scripts' folder!!!{Fore.RESET}{Back.RESET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files from which GAs are received, which are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m \u001b[1m DPR from BHUBANESHWAR received  \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m DPR from CUTTACK received  \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m DPR from JAMSHEDPUR received  \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m DPR from PATNA received  \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m DPR from RANCHI received  \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m DPR from VARANASI received  \u001b[39m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mail_to_gas = []\n",
    "new_file_list_lower=[]\n",
    "for i in new_file_list:\n",
    "    new_file_list_lower.append(i.lower())\n",
    "    \n",
    "new_file_list=new_file_list_lower\n",
    "\n",
    "for i in new_file_list:\n",
    "    print(f\"{Fore.GREEN} {Style.BRIGHT} DPR from {i.upper()} received  {Fore.RESET} {Style.RESET_ALL}\")\n",
    "    \n",
    "for i in old_file_name_has:\n",
    "    if i.lower() not in new_file_list:\n",
    "        mail_to_gas.append(i)\n",
    "        if i.lower()=='varanasi':\n",
    "            vd_count=1\n",
    "            vd_rem_count+=1\n",
    "        if i.lower()=='bhubaneshwar':\n",
    "            bd_count=1\n",
    "            bd_rem_count+=1\n",
    "        if i.lower()=='patna':\n",
    "            pd_count=1\n",
    "            pd_rem_count+=1\n",
    "        if i.lower()=='ranchi':\n",
    "            rd_count=1\n",
    "            rd_rem_count+=1\n",
    "        if i.lower()=='jamshedpur':\n",
    "            jd_count=1\n",
    "            jd_rem_count+=1\n",
    "        if i.lower()=='cuttack':\n",
    "            cd_count=1\n",
    "            cd_rem_count+=1\n",
    "        \n",
    "        if datetime.today().time().hour<13:\n",
    "            print(f\"{Fore.RED} {Style.BRIGHT} DPR from {i.upper()} not received. Not sending mail yet, since decided time is 1 PM.{Fore.RESET} {Style.RESET_ALL}\")\n",
    "        elif datetime.today().time().hour>=13:\n",
    "            print(f\"{Fore.RED} {Style.BRIGHT} DPR from {i.upper()} not received. Sending reminder mail.{Fore.RESET} {Style.RESET_ALL}\")\n",
    "            \n",
    "dpr_delay_in = open(\"dpr_delay_count.pickle\",'rb')\n",
    "dpr_delay_count= pickle.load(dpr_delay_in)\n",
    "if dpr_delay_count.iloc[-1,0]!=datetime.now().date() and datetime.today().time().hour>=13:\n",
    "#     dpr_delay_count=dpr_delay_count.append({'day':datetime.now().date(),'varanasi':vd_count,'bhubaneshwar':bd_count,'patna':pd_count,'ranchi':rd_count,'jamshedpur':jd_count,'cuttack':cd_count},ignore_index=True)\n",
    "    dpr_delay_count = pd.concat([dpr_delay_count, pd.DataFrame({'day':datetime.now().date(),'varanasi':vd_count,'bhubaneshwar':bd_count,'patna':pd_count,'ranchi':rd_count,'jamshedpur':jd_count,'cuttack':cd_count}, index=[0])], ignore_index=True)\n",
    "    dpr_delay_count.drop_duplicates(inplace=True)\n",
    "    pickle_dpr_delay_count= open(\"dpr_delay_count.pickle\",\"wb\")\n",
    "    pickle.dump(dpr_delay_count,pickle_dpr_delay_count)\n",
    "    pickle_dpr_delay_count.close()  \n",
    "dpr_delay_in.close()\n",
    "\n",
    "if run_only_if_all_files_are_present!=1:\n",
    "    dpr_reminder_count=pd.DataFrame(data={'varanasi':[vd_rem_count],'bhubaneshwar':[bd_rem_count],'patna':[pd_rem_count],'ranchi':[rd_rem_count],'jamshedpur':[jd_rem_count],'cuttack':[cd_rem_count]})\n",
    "    pickle_dpr_reminder_count= open(\"dpr_reminder_count.pickle\",\"wb\")\n",
    "    pickle.dump(dpr_reminder_count,pickle_dpr_reminder_count)\n",
    "    pickle_dpr_reminder_count.close()\n",
    "    \n",
    "if run_only_if_all_files_are_present==1:\n",
    "    dpr_reminder_count=pd.DataFrame(data={'varanasi':[0],'bhubaneshwar':[0],'patna':[0],'ranchi':[0],'jamshedpur':[0],'cuttack':[0]})\n",
    "    pickle_dpr_reminder_count= open(\"dpr_reminder_count.pickle\",\"wb\")\n",
    "    pickle.dump(dpr_reminder_count,pickle_dpr_reminder_count)\n",
    "    pickle_dpr_reminder_count.close()\n",
    "    \n",
    "if run_only_if_all_files_are_present==1:\n",
    "    if dpr_delay_count.iloc[-1,0]!=datetime.now().date() and datetime.today().time().hour<13:\n",
    "        dpr_delay_in = open(\"dpr_delay_count.pickle\",'rb')\n",
    "        dpr_delay_count= pickle.load(dpr_delay_in)\n",
    "#         dpr_delay_count=dpr_delay_count.append({'day':datetime.now().date(),'varanasi':0,'bhubaneshwar':0,'patna':0,'ranchi':0,'jamshedpur':0,'cuttack':0},ignore_index=True)\n",
    "        dpr_delay_count = pd.concat([dpr_delay_count, pd.DataFrame({'day':datetime.now().date(),'varanasi':0,'bhubaneshwar':0,'patna':0,'ranchi':0,'jamshedpur':0,'cuttack':0}, index=[0])], ignore_index=True)\n",
    "        dpr_delay_count.drop_duplicates(inplace=True)\n",
    "        pickle_dpr_delay_count= open(\"dpr_delay_count.pickle\",\"wb\")\n",
    "        pickle.dump(dpr_delay_count,pickle_dpr_delay_count)\n",
    "        pickle_dpr_delay_count.close()  \n",
    "        dpr_delay_in.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating REMINDER for non-receipt of DPR timely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reminder_number=0\n",
    "def threatning_mail(ga_name):\n",
    "    if datetime.today().time().hour>12:\n",
    "        ghante= datetime.today().time().hour-12\n",
    "    else:\n",
    "        ghante= datetime.today().time().hour\n",
    "    minute= datetime.today().minute\n",
    "\n",
    "    msg= EmailMessage()\n",
    "    sender= 'gailcgdwork@gmail.com'\n",
    "    msg['From']=sender\n",
    "    total_tracked_days=dpr_delay_count.shape[0]\n",
    "    \n",
    "    if ga_name.lower()== 'varanasi':\n",
    "        reminder_number= vd_rem_count\n",
    "        total_delays= dpr_delay_count.varanasi.sum()\n",
    "        msg['To']= mail_dict[\"mail_varanasi\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_varanasi\"][1:]\n",
    "    if ga_name.lower()== 'bhubaneshwar':\n",
    "        reminder_number= bd_rem_count\n",
    "        total_delays=dpr_delay_count.bhubaneshwar.sum()\n",
    "        msg['To']= mail_dict[\"mail_bhubaneshwar\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_bhubaneshwar\"][1:]\n",
    "    if ga_name.lower()== 'patna':\n",
    "        reminder_number= pd_rem_count\n",
    "        total_delays=dpr_delay_count.patna.sum()\n",
    "        msg['To']= mail_dict[\"mail_patna\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_patna\"][1:]\n",
    "    if ga_name.lower()== 'ranchi':\n",
    "        reminder_number= rd_rem_count\n",
    "        total_delays=dpr_delay_count.ranchi.sum()\n",
    "        msg['To']= mail_dict[\"mail_ranchi\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_ranchi\"][1:]\n",
    "    if ga_name.lower()== 'jamshedpur':\n",
    "        reminder_number= jd_rem_count\n",
    "        total_delays=dpr_delay_count.jamshedpur.sum()\n",
    "        msg['To']= mail_dict[\"mail_jamshedpur\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_jamshedpur\"][1:]\n",
    "    if ga_name.lower()== 'cuttack':\n",
    "        reminder_number= cd_rem_count\n",
    "        total_delays=dpr_delay_count.cuttack.sum()\n",
    "        msg['To']= mail_dict[\"mail_cuttack\"][0]\n",
    "        msg['CC']= mail_dict[\"mail_cuttack\"][1:]\n",
    "\n",
    "#     if ga_name.lower()== 'varanasi':\n",
    "#         reminder_number= vd_rem_count\n",
    "#         total_delays= dpr_delay_count.varanasi.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "#     if ga_name.lower()== 'bhubaneshwar':\n",
    "#         reminder_number= bd_rem_count\n",
    "#         total_delays=dpr_delay_count.bhubaneshwar.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "#     if ga_name.lower()== 'patna':\n",
    "#         reminder_number= pd_rem_count\n",
    "#         total_delays=dpr_delay_count.patna.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "#     if ga_name.lower()== 'ranchi':\n",
    "#         reminder_number= rd_rem_count\n",
    "#         total_delays=dpr_delay_count.ranchi.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "#     if ga_name.lower()== 'cuttack':\n",
    "#         reminder_number= cd_rem_count\n",
    "#         total_delays=dpr_delay_count.cuttack.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "#     if ga_name.lower()== 'jamshedpur':\n",
    "#         reminder_number= jd_rem_count\n",
    "#         total_delays=dpr_delay_count.jamshedpur.sum()\n",
    "#         msg['To']= mail_dict[\"test_mail\"][0]\n",
    "#         msg['CC']= mail_dict[\"test_mail\"][1:]\n",
    "\n",
    "        \n",
    "    msg['Subject']= f\"{ga_name.upper()}: AUTO-REMINDER - {reminder_number} FOR NON-RECEIPT OF DPR !!!\"\n",
    "    msg.set_content(f\"\"\"REMINDER #{reminder_number} FOR DPR!!!\\n\\nIt is {datetime.today().strftime(\"%I:%M %p\")} now. Please send the DPR timely.\\n\\nThe DPR reporting yesterday's progress has been delayed by {ghante-1} hours and {minute} minutes from the decided time of 1 PM. Please Send.\n",
    "\n",
    "NOTE: \n",
    "\\n1. Total days (\"recorded\"), when your GA delayed in sending DPR since 24th July 2022 is: {total_delays} days out of {total_tracked_days} days.\n",
    "\\n2. For now, this mail is only being sent internally. In case of repeated delays in receipt of DPR from your site, it shall be sent to your GAIL GA incharge as well.\n",
    "\\n3. DO NOT reply to this mail. This mail is meant to receive only your Site DPRs.\n",
    " \\n\\n \n",
    "IF YOU BELIEVE THIS MAIL REACHED YOU IN ERROR, PLEASE CONTACT THE UNDERSIGNED:\n",
    " \\n\n",
    "Regards,\n",
    "Nishant Sinha\n",
    "Deputy Manager\n",
    "  \n",
    "  \"\"\")\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com',465) as smtp:\n",
    "            smtp.login('gailcgdwork@gmail.com', '<YOUR PASSWORD HERE>')\n",
    "            smtp.send_message(msg)\n",
    "    print(f\"Reminder Mail sent successfully to {Fore.BLUE} {Style.BRIGHT}{ga_name}{Fore.RESET} {Style.RESET_ALL}\")\n",
    " \n",
    "   \n",
    "if mail_to_gas:\n",
    "    for ga in mail_to_gas:\n",
    "        if datetime.today().time().hour>=13:  #### to do unpickle the dpr_delay_count (check once, check for sum, add another mail function and call it if this value is greater than say 50)\n",
    "            threatning_mail(ga)\n",
    "        else:\n",
    "            pass\n",
    "    if datetime.today().time().hour<13:\n",
    "        dpr_reminder_count=pd.DataFrame(data={'varanasi':[0],'bhubaneshwar':[0],'patna':[0],'ranchi':[0],'jamshedpur':[0],'cuttack':[0]})\n",
    "        pickle_dpr_reminder_count= open(\"dpr_reminder_count.pickle\",\"wb\")\n",
    "        pickle.dump(dpr_reminder_count,pickle_dpr_reminder_count)\n",
    "        pickle_dpr_reminder_count.close()\n",
    "        \n",
    "        print(f\"{Fore.YELLOW} {Style.BRIGHT}Not sending any reminder for now, as fixed time is 1 PM everyday{Fore.RESET} {Style.RESET_ALL}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-establish the state of files, if all files were not received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_state():\n",
    "    msg = EmailMessage()\n",
    "    sender = 'gailcgdwork@gmail.com'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = sender\n",
    "    msg['Subject'] = 'DPR being resent (not all DPRs received from sites/ Error Occured)'\n",
    "    for i in os.listdir():\n",
    "        for j in old_file_name_has:\n",
    "            if j.lower() in i.lower() and '.xlsm' in i:\n",
    "                if j.lower() == i.split('_')[0].lower():\n",
    "                    try:\n",
    "                        if j.lower() == 'varanasi':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Varanasi_DPR.xlsm\", 'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Varanasi_DPR.xlsm\", f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_1}\")\n",
    "                        elif j.lower() == 'bhubaneshwar':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Bhubaneshwar_DPR.xlsm\", 'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Bhubaneshwar_DPR.xlsm\", f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_2}\")\n",
    "                        elif j.lower() == 'patna':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Patna_DPR.xlsm\", 'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Patna_DPR.xlsm\", f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_3}\")\n",
    "                        elif j.lower() == 'ranchi':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Ranchi_DPR.xlsm\", 'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Ranchi_DPR.xlsm\", f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_4}\")\n",
    "                        elif j.lower() == 'jamshedpur':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Jamshedpur_DPR.xlsm\",'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Jamshedpur_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_5}\")\n",
    "                        elif j.lower() == 'cuttack':\n",
    "                            with open(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Cuttack_DPR.xlsm\",'r') as file_handle:\n",
    "                                pass\n",
    "                            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Cuttack_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_6}\")\n",
    "                    except (PermissionError,FileNotFoundError):\n",
    "                        pass\n",
    "    for i in os.listdir():\n",
    "        for j in old_file_name_has:\n",
    "            if j.lower() in i.lower() and '.xlsm' in i:\n",
    "                with open(i, 'rb') as f:\n",
    "                    file_data= f.read()\n",
    "                    file_name= f.name   \n",
    "                    f.close() ###\n",
    "                msg.add_attachment(file_data,maintype='application',subtype='octet-stream',filename=file_name)\n",
    "                \n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com',465) as smtp:\n",
    "        smtp.login('gailcgdwork@gmail.com', '<YOUR PASSWORD HERE>')\n",
    "        smtp.send_message(msg)\n",
    "    os.chdir('C:\\\\Users\\\\hp\\\\Documents')\n",
    "    \n",
    "    shutil.copy(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\dpr_delay_count.pickle\",\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "    shutil.copy(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\dpr_reminder_count.pickle\",\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\")\n",
    "    \n",
    "    shutil.copy(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\dpr_delay_count.pickle\",\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\")\n",
    "    shutil.copy(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\dpr_reminder_count.pickle\",\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\")\n",
    "\n",
    "    \n",
    "    shutil.rmtree(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")\n",
    "    if user_input in ['y','Y']:\n",
    "        shutil.copytree(\"C:\\\\Users\\\\hp\\\\Desktop\\\\backup_of_python_script_folder\",\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")\n",
    "        print(f'The backup from {Fore.GREEN}\"CONSCIOUSLY\" {Fore.RESET}saved state has been restored')\n",
    "    else:\n",
    "        shutil.copytree(\"C:\\\\Users\\\\hp\\\\Pictures\\\\mand_back_py_script_folder\",\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")  \n",
    "        print(f'The backup from {Fore.YELLOW} \"UNCONSCIOUSLY\" {Fore.RESET}saved state has been restored')\n",
    "    os.chdir(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")\n",
    "    \n",
    "if run_only_if_all_files_are_present!=1:\n",
    "    restore_state()\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psutil\n",
    "# import os\n",
    "\n",
    "# def kill_process_using_file(file_path):\n",
    "#     for proc in psutil.process_iter():\n",
    "#         try:\n",
    "#             files = proc.open_files()\n",
    "#             for f in files:\n",
    "#                 if f.path == file_path:\n",
    "#                     proc.kill()\n",
    "#                     break\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "# file_path = \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Varanasi_DPR.xlsm\"\n",
    "# kill_process_using_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data Bank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Uncomment the following and run for any changes made to the Data Bank file, then comment again ###########\n",
    "\n",
    "# # Note: First convert the xlsb file to xlsm file\n",
    "\n",
    "# df_main = pd.read_excel(\"CGD_DPR_DATA_BANK.xlsm\",sheet_name=\"Data\")\n",
    "# df_main.columns= df_main.iloc[0]\n",
    "# df_main= df_main[1:]\n",
    "\n",
    "# pickle_out= open(\"df_main.pickle\",\"wb\")\n",
    "# pickle.dump(df_main,pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"df_main.pickle\",'rb')\n",
    "df_main= pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPR engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431700d702574ea89eb80b8ed861d389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Bhubaneshwar \u001b[39m \u001b[0m \n",
      "simplified: 78.417, oversimplified: 78.417\n",
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Cuttack \u001b[39m \u001b[0m \n",
      "simplified: 52.224, oversimplified: 52.224000000000004\n",
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Jamshedpur \u001b[39m \u001b[0m \n",
      "simplified: 184.214, oversimplified: 184.214\n",
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Patna \u001b[39m \u001b[0m \n",
      "simplified: 152.421, oversimplified: 152.421\n",
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Ranchi \u001b[39m \u001b[0m \n",
      "simplified: 630, oversimplified: 630\n",
      "\u001b[32m \u001b[1m No mismatch in total for simplified and oversimplified file: Varanasi \u001b[39m \u001b[0m \n",
      "simplified: 3384.3, oversimplified: 3384.3\n"
     ]
    }
   ],
   "source": [
    "mismatch_track=0\n",
    "if (run_only_if_all_files_are_present==1) and (len(duplicacy_check) == 0):\n",
    "    final_combined_df = pd.DataFrame()\n",
    "    must_contain_string= ['riser','meter','20','32','63','90','125','conversion','charged','steel_4','steel_6','steel_8','steel_12','weld_4','weld_6','weld_8','weld_12','comm_4','comm_6','comm_8','comm_12']  \n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"OVERSIMPLIFIED_FILES\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(\"SIMPLIFIED_FILES\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(\"COMBINED_DPR\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def for_steel_sheet():\n",
    "        column_stop_here= ''\n",
    "        global df2_steel,df_steel\n",
    "        df_steel= pd.read_excel(file_name,sheet_name=sheet_name)\n",
    "        df_steel= df_steel.iloc[df_steel[df_steel['Unnamed: 3'].str.contains('Progress',case=False,regex=True)==True].index.min():,:]\n",
    "        df_steel.drop([3,5],inplace=True)                               #----------------> Be cautious here\n",
    "        df_steel.iloc[0]=df_steel.iloc[0].combine_first(df_steel.iloc[1])\n",
    "        df_steel.columns= df_steel.iloc[0]\n",
    "\n",
    "        lower_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False))[0]+1\n",
    "        upper_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False))[0]+6\n",
    "        df_steel_1= df_steel.iloc[lower_index: upper_index]\n",
    "\n",
    "        lower_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"lowering\",case=False))[0]+1\n",
    "        upper_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"lowering\",case=False))[0]+6\n",
    "#         df_steel_1= df_steel_1.append(df_steel.iloc[lower_index: upper_index])\n",
    "        df_steel_1 = pd.concat([df_steel_1, df_steel.iloc[lower_index: upper_index]], ignore_index=True)\n",
    "\n",
    "    \n",
    "        lower_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"Commissioning\",case=False))[0]+1 ### *(*(*()))\n",
    "        upper_index= np.flatnonzero(df_steel['Activity Description'].dropna().str.contains(\"Commissioning\",case=False))[0]+6 ### *(*(*()))\n",
    "#         df_steel_1= df_steel_1.append(df_steel.iloc[lower_index: upper_index])  ### *(*(*()))\n",
    "        df_steel_1 = pd.concat([df_steel_1, df_steel.iloc[lower_index: upper_index]], axis=0, ignore_index=True)\n",
    "        df_steel= df_steel_1\n",
    "        ################################################# OR  #######################################################\n",
    "        # lower_index=df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False).index[df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False)][0]\n",
    "        # upper_index=df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False).index[df_steel['Activity Description'].dropna().str.contains(\"welding\",case=False)][0]+4\n",
    "        # df_steel.loc[lower_index:upper_index,:]\n",
    "        #############################################################################################################\n",
    "        for x in df_steel.columns.unique():\n",
    "            if \"scope\" in str(x).lower():                         #----------------> Last word must have \"inch\" somewhere\n",
    "                    column_stop_here = x\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        df_steel= df_steel.loc[:,: column_stop_here]\n",
    "        df_steel= df_steel.set_index(df_steel.iloc[:,1])\n",
    "        df_steel=df_steel.iloc[:,2:df_steel.shape[1]-1]\n",
    "\n",
    "        df_steel.columns.name= \"Column_Description\"\n",
    "\n",
    "        df1_steel= df_steel.iloc[:,df_steel.columns==\"today\"].iloc[:,0:]\n",
    "        df1_steel.replace(0, np.nan, inplace=True)\n",
    "        df1_steel.columns= df_steel.columns[1::4][0:df1_steel.columns.shape[0]]\n",
    "\n",
    "        df2_steel= df1_steel.loc[:,~df1_steel.isna().all()].T\n",
    "        df2_steel.dropna(axis=1, how='all',inplace=True)\n",
    "        df2_steel= df2_steel.iloc[:,np.flatnonzero(df2_steel.columns.str.contains('weld_|steel_|comm_',case=False))]\n",
    "        return df2_steel,df_steel\n",
    "\n",
    "    def for_mdpe_sheet():\n",
    "        column_stop_here= ''\n",
    "        global df2,df\n",
    "        df= pd.read_excel(file_name,sheet_name=sheet_name)\n",
    "        df= df.iloc[df[df['Unnamed: 1'].str.contains('Description|Activity',case=False,regex=True)==True].index.min():,:]\n",
    "        df.drop([4,5],inplace=True)                               #----------------> Be cautious here\n",
    "        df.iloc[0]=df.iloc[0].combine_first(df.iloc[1])\n",
    "        df.columns= df.iloc[0]\n",
    "        df.drop([3,6,7,13,17,19,20], inplace=True)    ### *(*(*(           #----------------> Be cautious here... 20 included for on gas \n",
    "        row_stop_here= df.iloc[:,:3].dropna(thresh=3).shape[0]   #----------------> assuming first 3 values are not blank\n",
    "        for i in df.columns.unique():\n",
    "            if \"inch\" in str(i).lower():                         #----------------> Last word must have \"inch\" somewhere\n",
    "                    column_stop_here = i\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        df= df.loc[:,: column_stop_here]\n",
    "        df= df.iloc[0:row_stop_here,1:]\n",
    "\n",
    "    #                 df= df.iloc[3:,1:]\n",
    "    #                 df.drop([4,5],inplace=True)\n",
    "    #                 df.iloc[0]= df.iloc[0].combine_first(df.iloc[1])\n",
    "    #                 df.columns= df.iloc[0]\n",
    "    #                 df.drop([3,6,7,13,14,17,19], inplace=True)\n",
    "    #                 df= df.loc[:18,: ' INCH-KM']\n",
    "\n",
    "\n",
    "        df= df.set_index(df.iloc[:,0])\n",
    "        df.drop(df.columns[0],axis=1,inplace=True)\n",
    "        df.columns.name= \"Column_Description\"\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "        df1= df.iloc[:,df.columns==DATE_IN_DPR].iloc[:,0:]\n",
    "        df1.replace(0, np.nan, inplace=True)\n",
    "        df1.columns= df.columns[1::4][0:df1.columns.shape[0]]\n",
    "\n",
    "    ###############################################################################\n",
    "\n",
    "        df2= df1.loc[:,~df1.isna().all()].T\n",
    "        df2.dropna(axis=1, how='all',inplace=True)\n",
    "        return df2,df\n",
    "\n",
    "\n",
    "\n",
    "    for file_name in tqdm(os.listdir()):\n",
    "        file_name_lower= file_name.lower()\n",
    "        condition1= '.xlsx' in file_name_lower or '.xls' in file_name_lower or \".xlsm\" in file_name_lower\n",
    "        condition2= \"~\" not in file_name_lower\n",
    "        condition3= \"cgd_dpr_data_bank\" not in file_name_lower\n",
    "        condition4= 'nis' not in file_name\n",
    "        condition5= 'DPR_MACRO' not in file_name_lower\n",
    "        condition= [condition1,condition2,condition3,condition4, condition5]\n",
    "        both_sheets_flag= 0\n",
    "        if all(condition):\n",
    "            xls = pd.ExcelFile(file_name)\n",
    "            no_progress_report=0\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                if \"steel\" in sheet_name.lower():                                 #-------------------------> Note here\n",
    "                    df2_steel,df_steel= for_steel_sheet()\n",
    "                    \n",
    "                    # first run me in tino blocks ko comment karna hai (so that initial files generate hon). New contractor add \n",
    "                    # hota hai to error aayega (size mismatch). New vendor include karte samay, in tino block ko comment karna hoga\n",
    "                    ### updated on 26-04-2023 (several vendors added, no error reported even with new vendor inclusion)\n",
    "              \n",
    "             ## Block 1\n",
    "                    df_previous_steel= df_steel.iloc[:,df_steel.columns=='Previous'].iloc[:,0:]\n",
    "                    df_previous_steel.replace(0, np.nan, inplace=True)\n",
    "                    df_previous_steel.columns= df_steel.columns[1::4][0:df_previous_steel.columns.shape[0]]  ## aaj ka previous value\n",
    "            ## Block 2\n",
    "                    pickle_in_steel = open(f\"{file_name.split('_')[0].lower()}_steel.pickle\",'rb')\n",
    "                    df_cumulative_steel= pickle.load(pickle_in_steel) ## beete kal ka cumulative value  \n",
    "                    pickle_in_steel.close()  ##---##\n",
    "            ## Block 3\n",
    "                    mischief_steel= abs(df_cumulative_steel.replace(np.nan,0,inplace=False)-df_previous_steel.replace(np.nan,0,inplace=False)).sum()>0.001\n",
    "                    mischief_steel=mischief_steel[mischief_steel==True]\n",
    "                    if mischief_steel.sum()>0:\n",
    "                        x= abs(df_cumulative_steel[mischief_steel.index]-df_previous_steel[mischief_steel.index])>0\n",
    "                        for i in range(x.shape[1]):\n",
    "                            y= x[x.columns[i]]\n",
    "                        \n",
    "                        print(f\"\\n {Back.RED} Probable manipulation noted! Check: {Back.RESET} {Fore.RED} {Style.BRIGHT} {y[y].dropna().index.values} for the vendors: {y.name} in DPR received from {file_name.split('_')[0]} {Fore.RESET} {Style.RESET_ALL}\\n\")\n",
    "#                         print(abs((df_cumulative_steel-df_previous_steel).sum()))\n",
    "                    else:\n",
    "                        pass\n",
    "            ## Block 4        \n",
    "                    \n",
    "                    df_cumulative_steel= df_steel.iloc[:,df_steel.columns=='Cumulative'].iloc[:,0:]\n",
    "                    df_cumulative_steel.replace(0, np.nan, inplace=True)\n",
    "                    df_cumulative_steel.columns= df_steel.columns[1::4][0:df_cumulative_steel.columns.shape[0]]\n",
    "                    pickle_steel= open(f\"{file_name.split('_')[0].lower()}_steel.pickle\",\"wb\")\n",
    "                    pickle.dump(df_cumulative_steel,pickle_steel)\n",
    "                    pickle_steel.close()\n",
    "                              \n",
    "                    both_sheets_flag+=1\n",
    "                    continue\n",
    "                              \n",
    "                              \n",
    "                if \"mdpe\" in sheet_name.lower():                                 #-------------------------> Note here\n",
    "                    df2,df= for_mdpe_sheet()\n",
    "                    \n",
    "                    df_previous_mdpe= df.iloc[:,df.columns=='Previous'].iloc[:,0:]\n",
    "                    df_previous_mdpe.replace(0, np.nan, inplace=True)\n",
    "                    df_previous_mdpe.columns= df.columns[1::4][0:df_previous_mdpe.columns.shape[0]]  ## aaj ka previous value\n",
    "\n",
    "                    pickle_in_mdpe = open(f\"{file_name.split('_')[0].lower()}_mdpe.pickle\",'rb')\n",
    "                    df_cumulative_mdpe= pickle.load(pickle_in_mdpe) ## beete kal ka cumulative value\n",
    "                    pickle_in_mdpe.close()  ##-----##\n",
    "                    \n",
    "                    mischief_mdpe= abs((df_cumulative_mdpe.replace(np.nan,0,inplace=False)-df_previous_mdpe.replace(np.nan,0,inplace=False)).sum())>0.001\n",
    "                    mischief_mdpe=mischief_mdpe[mischief_mdpe==True]\n",
    "                    if mischief_mdpe.sum()>0:\n",
    "                        x= abs(df_cumulative_mdpe[mischief_mdpe.index]-df_previous_mdpe[mischief_mdpe.index])>0\n",
    "                        for i in range(x.shape[1]):\n",
    "                            y= x[x.columns[i]]\n",
    "                            print(f\"\\n {Back.RED} Probable manipulation noted! Check: {Back.RESET}{Fore.RED} {Style.BRIGHT} {y[y].dropna().index.values} for the vendors: {y.name} in DPR received from {file_name.split('_')[0]} {Fore.RESET} {Style.RESET_ALL}\\n\")\n",
    "#                         print(abs((df_cumulative_mdpe-df_previous_mdpe).sum()))\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    \n",
    "                    df_cumulative_mdpe= df.iloc[:,df.columns=='Cumulative'].iloc[:,0:]\n",
    "                    df_cumulative_mdpe.replace(0, np.nan, inplace=True)\n",
    "                    df_cumulative_mdpe.columns= df.columns[1::4][0:df.columns.shape[0]][0:-2]\n",
    "                    pickle_mdpe= open(f\"{file_name.split('_')[0].lower()}_mdpe.pickle\",\"wb\")\n",
    "                    pickle.dump(df_cumulative_mdpe,pickle_mdpe)\n",
    "                    pickle_mdpe.close()\n",
    "                              \n",
    "                              \n",
    "                    both_sheets_flag+=1\n",
    "                    continue\n",
    "                if both_sheets_flag==2:    \n",
    "                    df2= pd.concat([df2,df2_steel],sort=False)\n",
    "\n",
    "      ################ try except added, so that with 0 progress, no error comes up ###########              \n",
    "                    try:\n",
    "                        df2.loc[\"Total\"]= df2.fillna(0).sum()\n",
    "                    except ValueError:\n",
    "                        if no_progress_report<1:\n",
    "                            print(f\"{Fore.RED}{Style.BRIGHT}{Back.GREEN}No progress reported from {file_name.split('_')[0]} ? {Fore.RESET}{Style.RESET_ALL}{Back.RESET}\")\n",
    "                            no_progress_report+=1\n",
    "                            mismatch_track+=1\n",
    "                        pass\n",
    "\n",
    "      ################### if this breaks, delete try and except; keep df2.loc in line with df2= pd.concat ######\n",
    "\n",
    "\n",
    "                    df2.rename(columns={'Commissioning':'Charged'},inplace=True)\n",
    "                    df2.to_excel(f\"{os.getcwd()}\\\\SIMPLIFIED_FILES\\\\{file_name.split('_')[0]}_nis_simplified.xlsx\", \\\n",
    "                                      file_name.split('_')[0])\n",
    "                    final_df= pd.DataFrame()\n",
    "                    index= 0\n",
    "                    count=0\n",
    "                    for city_name in df_main.CITY.unique():\n",
    "                        if (type(city_name)!=float) and file_name.split(\"_\")[0].lower()==city_name.lower():\n",
    "                            df_varanasi = df_main[df_main.CITY==city_name]\n",
    "\n",
    "                            for i in range(df2.shape[0]):\n",
    "                                df_vns_1= df_varanasi[df_varanasi.CONTRACTOR==df2.iloc[i:i+1].index[0]]\n",
    "                                for j in range(len(df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)])):\n",
    "                                    value= df2.loc[df2.iloc[i:i+1].index[0],df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)][j]]\n",
    "                                    if  np.isnan(value).all():\n",
    "                                        pass\n",
    "                                    else:\n",
    "                 #######################################################   Changes made in this part\n",
    "                ######################## when a vendor is repeated in 2 sheets ###########################\n",
    "                                        if len(np.array(value).shape)!=0:\n",
    "                                            for k in value:\n",
    "                                                if np.isnan(k):\n",
    "                                                    pass\n",
    "                                                else:\n",
    "                 #######################################################                       \n",
    "                                                    try:\n",
    "                                                        if k<0:\n",
    "                                                            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Negative value({k}) entered for {df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)][j]} of contractor {df2.loc[df2.iloc[i:i+1].index[0]].name} {Fore.RESET} {Style.RESET_ALL} in {city_name} \")\n",
    "                                                    except IndexError:\n",
    "                                                        pass\n",
    "                                                    z = df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)][j].split(' ')\n",
    "                                                    z_lower= [n.lower() for n in z] \n",
    "                                                    nish=0\n",
    "                                                    for val in z_lower:\n",
    "                                                        if val in must_contain_string:\n",
    "                                                            nish= val\n",
    "                                                    if_else_run_track=0\n",
    "                                                    if nish in must_contain_string[9:]:\n",
    "                                                        nish_1= nish.split('_')[0]\n",
    "                                                        if nish_1=='comm':\n",
    "                                                            nish_1='charged'\n",
    "                                                        nish_2= nish.split('_')[1]\n",
    "                                                        if df_vns_1.ACTIVITY.str.contains(nish_1,case=False).sum()>0:\n",
    "                                                            if df_vns_1.SIZE.str.contains(nish_2,case=False).sum()>0:\n",
    "#                                                                 final_df= final_df.append(df_vns_1[(df_vns_1.ACTIVITY.str.contains(nish_1,case=False) & df_vns_1.SIZE.str.contains(nish_2,case=False))==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                                final_df = pd.concat([final_df, df_vns_1[(df_vns_1['ACTIVITY'].str.contains(nish_1, case=False) & df_vns_1['SIZE'].str.contains(nish_2, case=False))==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "                                                                final_df.iloc[index,7]= k\n",
    "                                                                index+=1\n",
    "                                                                if_else_run_track=1\n",
    "\n",
    "\n",
    "                                                    elif df_vns_1.ACTIVITY.str.contains(nish,case=False).sum()>0:\n",
    "#                                                         final_df= final_df.append(df_vns_1[df_vns_1.ACTIVITY.str.contains(nish,case=False)==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                        final_df = pd.concat([final_df, df_vns_1[df_vns_1['ACTIVITY'].str.contains(nish, case=False)==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "\n",
    "                                                        final_df.iloc[index,7]= k\n",
    "                                                        index+=1\n",
    "                                                        if_else_run_track=1\n",
    "\n",
    "                                                    elif df_vns_1.SIZE.str.contains(nish,case=False).sum()>0:\n",
    "#                                                         final_df= final_df.append( df_vns_1[df_vns_1.SIZE.str.contains(nish,case=False)==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                        final_df = pd.concat([final_df, df_vns_1[df_vns_1['SIZE'].str.contains(nish, case=False)==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "\n",
    "                                                        final_df.iloc[index,7]= k\n",
    "                                                        index+=1\n",
    "                                                        if_else_run_track=1\n",
    "\n",
    "                                        else:\n",
    "                                            try:\n",
    "                                                if value<0:\n",
    "                                                    print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Negative value({value}) entered for {df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)][j]} of contractor {df2.loc[df2.iloc[i:i+1].index[0]].name} {Fore.RESET} {Style.RESET_ALL} in {city_name} \")\n",
    "                                            except IndexError:\n",
    "                                                pass\n",
    "                                            z = df2.columns[(df2.iloc[i:i+1]!=0).all(axis=0)][j].split(' ')\n",
    "                                            z_lower= [n.lower() for n in z] \n",
    "                                            nish=0\n",
    "                                            for val in z_lower:\n",
    "                                                if val in must_contain_string:\n",
    "                                                    nish= val\n",
    "                                            if_else_run_track=0\n",
    "                                            if nish in must_contain_string[9:]:\n",
    "                                                nish_1= nish.split('_')[0]\n",
    "                                                if nish_1=='comm':\n",
    "                                                    nish_1='charged'\n",
    "                                                nish_2= nish.split('_')[1]\n",
    "                                                if df_vns_1.ACTIVITY.str.contains(nish_1,case=False).sum()>0:\n",
    "                                                    if df_vns_1.SIZE.str.contains(nish_2,case=False).sum()>0:\n",
    "#                                                         final_df= final_df.append(df_vns_1[(df_vns_1.ACTIVITY.str.contains(nish_1,case=False) & df_vns_1.SIZE.str.contains(nish_2,case=False))==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                        final_df = pd.concat([final_df, df_vns_1[(df_vns_1['ACTIVITY'].str.contains(nish_1, case=False) & df_vns_1['SIZE'].str.contains(nish_2, case=False))==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "\n",
    "                                                        final_df.iloc[index,7]= value\n",
    "                                                        index+=1\n",
    "                                                        if_else_run_track=1\n",
    "\n",
    "\n",
    "                                            elif df_vns_1.ACTIVITY.str.contains(nish,case=False).sum()>0:\n",
    "#                                                 final_df= final_df.append(df_vns_1[df_vns_1.ACTIVITY.str.contains(nish,case=False)==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                final_df = pd.concat([final_df, df_vns_1[df_vns_1['ACTIVITY'].str.contains(nish, case=False)==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "\n",
    "                                                final_df.iloc[index,7]= value\n",
    "                                                index+=1\n",
    "                                                if_else_run_track=1\n",
    "\n",
    "                                            elif df_vns_1.SIZE.str.contains(nish,case=False).sum()>0:\n",
    "#                                                 final_df= final_df.append( df_vns_1[df_vns_1.SIZE.str.contains(nish,case=False)==True].iloc[0:1],ignore_index=True,sort=False)\n",
    "                                                final_df = pd.concat([final_df, df_vns_1[df_vns_1['SIZE'].str.contains(nish, case=False)==True].iloc[0:1]], ignore_index=True, sort=False)\n",
    "\n",
    "                                                final_df.iloc[index,7]= value\n",
    "                                                index+=1\n",
    "                                                if_else_run_track=1\n",
    "\n",
    "\n",
    "\n",
    "                                if if_else_run_track==1:\n",
    "                                    yesterday= datetime.strftime(datetime.now()-timedelta(1), \"%d-%b-%y\")  \n",
    "                                    ubab=final_df.iloc[:,2:8]              ####### Added to drop any duplicates (checking values in column 2 to 8)\n",
    "                                    \n",
    "                                    if len(ubab)-len(ubab.drop_duplicates())!=0:\n",
    "                                        index= index- (len(ubab)-len(ubab.drop_duplicates()))\n",
    "                                    \n",
    "                                    ubab.drop_duplicates(inplace=True)\n",
    "                                    final_df=final_df.iloc[ubab.index,:]\n",
    "                                    final_df.DATE= yesterday \n",
    "\n",
    "                                    ##### THE VALUES THAT HAS TO BE DELETED IN THE OVERSIMPLIFIED FILE######\n",
    "                                    final_df.Awarded= np.nan\n",
    "                                    final_df[\"MoU Target 21-22\"]= np.nan\n",
    "                                    final_df[\"Target\"]=np.nan\n",
    "                                    final_df[\"5 Years Targets\"]=np.nan\n",
    "    \n",
    "                                    ########################################################################\n",
    "                                    final_df= final_df.iloc[:,0:41]\n",
    "    \n",
    "    \n",
    "    \n",
    "        #                             final_df['DATE']='=today()-1'\n",
    "        #                             final_df['PROGRESS Inch-Mtr']= '=IF(OR([@ACTIVITY]=\"Steel Laying\", [@ACTIVITY]=\"PE Laying\"), [@[PROGRESS Mtr]]*[@[Inch Multiplier]], 0)'\n",
    "        #                             final_df['Pipe size unit']='=IF(RIGHT([@SIZE],1)=\"m\",\"mm\",\"\"\"\")'\n",
    "        #                             final_df['Check1']='=LEN(K43836)'\n",
    "        #                             final_df['Pipe size value']='=LEFT([@SIZE],(LEN([@SIZE])-L43836))'\n",
    "        #                             final_df['Inch Multiplier']='=IF([@ACTIVITY]=\"PE Laying\", [@[Pipe size value]]/25.4, [@[Pipe size value]])'\n",
    "        #                             final_df['Check 3']='=$C$1-[@DATE]+1'\n",
    "        #                             final_df['PROGRESS Mtr (Today)']='=IF([@[Check 3]]<2,[@[PROGRESS Mtr]],0)'\n",
    "        #                             final_df[\"TODAY\\'s PROGRESS INCH-M\"]='=IF([@[Check 3]]<2, [@[PROGRESS Inch-Mtr]], 0)'\n",
    "        #                             final_df['Today\\'s Progress (All)']='=IF([@Project]= \"STEEL +MDPE\", [@[Total Inch-Km Progress Today]], [@[PROGRESS Mtr (Today)]])'\n",
    "        #                             final_df['PROGRESS Mtr (Last 7 days)2']='=IF([@[Check 3]]<8,[@[PROGRESS Mtr]],0)'\n",
    "        #                             final_df['PROGRESS Mtr (Last 7 days) INCH-KM']='=@IF([@[PROGRESS Mtr (Last 7 days)2]]=0,0,I:I)'\n",
    "        #                             final_df['PROGRESS Mtr (last 15 days)']='=IF([@[Check 3]]<16,[@[PROGRESS Mtr]],0)'\n",
    "        #                             final_df['PROGRESS Mtr (last 15 days) INCH-KM']='=@IF([@[PROGRESS Mtr (last 15 days)]]=0,0,I:I)'\n",
    "        #                             final_df['PROGRESS Mtr (last 30 days)']='=IF([@[Check 3]]<31,[@[PROGRESS Mtr]],0)'\n",
    "        #                             final_df['PROGRESS Mtr (last 30 days) INCH-KM']='=@IF([@[PROGRESS Mtr (last 30 days)]]=0,0,I:I)'\n",
    "        #                             final_df['PROGRESS Mtr (last 90 days)']='=IF([@[Check 3]]<91,[@[PROGRESS Mtr]],0)'\n",
    "        #                             final_df['PROGRESS Mtr (last 90 days) INCH-KM']='=@IF([@[PROGRESS Mtr (last 90 days)]]=0,0,I:I)'\n",
    "        #                             final_df['Filter for Previous FY']='=IF([@DATE]<$AV$2, \"Previous FY\", \"Current FY\")'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                                    writer = pd.ExcelWriter(f\"{os.getcwd()}\\\\OVERSIMPLIFIED_FILES\\\\{file_name.split('_')[0]}_nis_oversimplified.xlsx\", engine='xlsxwriter')\n",
    "                                    final_df.to_excel(writer,sheet_name= file_name.split('_')[0], index=False)\n",
    "    \n",
    "                                    (max_row, max_col) = final_df.shape\n",
    "                                    workbook = writer.book\n",
    "                                    worksheet = writer.sheets[file_name.split('_')[0]]\n",
    "    \n",
    "                                    worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': column_settings})\n",
    "                                    column_settings = [{'header': column} for column in final_df.columns]\n",
    "                                    worksheet.set_column(0, max_col - 1, 12)                                       \n",
    "    \n",
    "    \n",
    "    \n",
    "                                    writer.save()\n",
    "                                else:\n",
    "                                    pass\n",
    "                            if final_df.empty==False:\n",
    "                                simplified_sum_for_check= df2.iloc[-1:].fillna(0).sum().sum() # to check the total sum for simplified and oversimplified\n",
    "                                oversimplified_sum_for_check = final_df['PROGRESS Mtr'].sum()\n",
    "                            \n",
    "                                if abs(simplified_sum_for_check-oversimplified_sum_for_check) <=0.0001:\n",
    "                                    print(f\"{Fore.GREEN} {Style.BRIGHT} No mismatch in total for simplified and oversimplified file: {file_name.split('_')[0]} {Fore.RESET} {Style.RESET_ALL} \")\n",
    "                                    print(f\"simplified: {simplified_sum_for_check}, oversimplified: {oversimplified_sum_for_check}\")\n",
    "                                    mismatch_track+=1\n",
    "                                else:\n",
    "                                    print(f\"{Fore.CYAN} {Style.BRIGHT} Warning: MATCH OVERSIMPLIFIED AND SIMPLIFIED FILE FOR DIFFERENCE{Fore.MAGENTA} (check contractor scope!){Fore.RESET} for GA: {file_name.split('_')[0]} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "                                    print(f\"{Fore.YELLOW} {Back.BLACK} {Style.NORMAL} simplified: {simplified_sum_for_check}, oversimplified: {oversimplified_sum_for_check} {Fore.RESET} {Back.RESET} {Style.RESET_ALL}\")\n",
    "\n",
    "#                                 final_combined_df= final_combined_df.append(final_df)\n",
    "                                final_combined_df = pd.concat([final_combined_df, final_df], ignore_index=True, sort=False)\n",
    "\n",
    "                                both_sheets_flag=0\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "\n",
    "    ################## THOSE THAT HAS \"PE Laying\" values in KM, change them here to M ##############\n",
    "    if final_combined_df.empty==False:\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"PE Laying\") & (final_combined_df[\"CITY\"]=='Patna'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"PE Laying\") & (final_combined_df[\"CITY\"]=='Jamshedpur'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Steel Laying\") & (final_combined_df[\"CITY\"]=='Jamshedpur'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Steel Laying\") & (final_combined_df[\"CITY\"]=='Patna'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Welding\") & (final_combined_df[\"CITY\"]=='Jamshedpur'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Welding\") & (final_combined_df[\"CITY\"]=='Patna'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"PE Laying\") & (final_combined_df[\"CITY\"]=='Bhubaneshwar'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Welding\") & (final_combined_df[\"CITY\"]=='Bhubaneshwar'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Steel Laying\") & (final_combined_df[\"CITY\"]=='Bhubaneshwar'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"PE Laying\") & (final_combined_df[\"CITY\"]=='Cuttack'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Welding\") & (final_combined_df[\"CITY\"]=='Cuttack'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Steel Laying\") & (final_combined_df[\"CITY\"]=='Cuttack'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Charged\") & (final_combined_df[\"CITY\"]=='Patna'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Charged\") & (final_combined_df[\"CITY\"]=='Jamshedpur'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Charged\") & (final_combined_df[\"CITY\"]=='Cuttack'),'PROGRESS Mtr']*= 1000\n",
    "        final_combined_df.loc[(final_combined_df[\"ACTIVITY\"]==\"Charged\") & (final_combined_df[\"CITY\"]=='Bhubaneshwar'),'PROGRESS Mtr']*= 1000\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "    filename=f\"{os.getcwd()}\\\\OVERSIMPLIFIED_FILES\\\\all_nis_oversimplified.xlsx\"\n",
    "    writer= pd.ExcelWriter(filename,engine= 'xlsxwriter')                   \n",
    "    final_combined_df.dropna(subset=['PROGRESS Mtr'],inplace=True)\n",
    "    final_combined_df= final_combined_df.iloc[:,0:41]\n",
    "    final_combined_df.to_excel(writer,sheet_name= \"ALL CITIES\", index=False)\n",
    "    (max_row, max_col) = final_combined_df.shape\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[\"ALL CITIES\"]\n",
    "    worksheet.add_table(0, 0, max_row, max_col - 1, {'columns': column_settings})\n",
    "    column_settings = [{'header': column} for column in final_combined_df.columns]\n",
    "    worksheet.set_column(0, max_col - 1, 12)\n",
    "\n",
    "    # filename_macro= filename.split('.')[0]+'.xlsm'\n",
    "    # workbook= writer.book\n",
    "    # workbook.filename= filename_macro\n",
    "    # workbook.add_vba_project('vbaProject.bin')\n",
    "\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "else:\n",
    "    print(\"Check the folder: Not all the files have been received / duplicate files received\")\n",
    "try:\n",
    "    pickle_in.close()\n",
    "    pickle_in_steel.close()\n",
    "    pickle_in_mdpe.close()\n",
    "except NameError:\n",
    "    pass\n",
    "if mismatch_track!=6:\n",
    "    try:\n",
    "        os.mkdir(\"C:\\\\Users\\\\hp\\\\Pictures\\\\failed_stage_backup\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    shutil.rmtree(\"C:\\\\Users\\\\hp\\\\Pictures\\\\failed_stage_backup\")\n",
    "    shutil.copytree(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\",\"C:\\\\Users\\\\hp\\\\Pictures\\\\failed_stage_backup\")\n",
    "\n",
    "try:\n",
    "    del xls\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To restore the previous state forcefully, call the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_state():\n",
    "    try:\n",
    "        pickle_in.close()\n",
    "        pickle_in_steel.close()\n",
    "        pickle_in_mdpe.close()\n",
    "        restore_state()\n",
    "    except NameError:\n",
    "        pickle_in.close()\n",
    "        restore_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking care of the Data Bank file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 109 ms\n",
      "Wall time: 2min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "if mismatch_track==6:\n",
    "    if (run_only_if_all_files_are_present==1) and (len(duplicacy_check) == 0):\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Varanasi_DPR.xlsm\"')\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Bhubaneshwar_DPR.xlsm\"')\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Patna_DPR.xlsm\"')\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Ranchi_DPR.xlsm\"')\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Jamshedpur_DPR.xlsm\"')\n",
    "        os.system('start excel.exe \"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Cuttack_DPR.xlsm\"')\n",
    "        \n",
    "        sleep(3)\n",
    "        os.system('start chrome.exe')     \n",
    "#         excel = win32com.client.Dispatch(\"Excel.Application\")\n",
    "        \n",
    "#         hwnd = excel.Hwnd\n",
    "        \n",
    "#         win32gui.ShowWindow(hwnd, win32con.SW_MINIMIZE)\n",
    "\n",
    "        \n",
    "        xl= win32com.client.Dispatch('Excel.Application')\n",
    "        xl.DisplayAlerts = False\n",
    "        xl.Workbooks.Open(Filename=\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\DPR_MACRO.xlsm\",ReadOnly=1)\n",
    "        sleep(10)\n",
    "        xl.Application.Run(\"DPR_MACRO.xlsm!Module1.Macro1\")\n",
    "        excel = win32com.client.GetObject(None, \"Excel.Application\")\n",
    "        for window in excel.Windows:\n",
    "            if window.Caption.startswith(\"Microsoft Excel\"):\n",
    "                window.Close()\n",
    "                \n",
    "        xl.Application.Quit()\n",
    "        del xl\n",
    "        os.system(\"\"\"taskkill /f /im excel.exe\"\"\")\n",
    "    else:\n",
    "        print(\"Macros won't run, since all files have not been received.\")\n",
    "else:\n",
    "    print(\"There seems to be some mismatch in the Oversimplified and Simplified file. Please check for mistakes.\")\n",
    "\n",
    "#### If any instance of excel is still open, I am using the below to kill all instances of excel:\n",
    "\n",
    "def close_excel_instances():\n",
    "    try:\n",
    "        xl = win32com.client.GetObject(None, \"Excel.Application\")\n",
    "        xl.DisplayAlerts = False\n",
    "        xl.AlertBeforeOverwriting = False\n",
    "        for process in psutil.process_iter():\n",
    "            if process.name() == \"EXCEL.EXE\":\n",
    "                process.kill()\n",
    "    except:\n",
    "        pass\n",
    "close_excel_instances()\n",
    "\n",
    "os.system(\"\"\"taskkill /f /im chrome.exe\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending file to \"nishantsinha0502@gmail.com\" for review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\HM6176\\\\hp\\\\Documents\\\\Python Scripts\\\\COMBINED_DPR'\n",
      "C:\\Users\\hp\\Documents\\Python Scripts\n"
     ]
    }
   ],
   "source": [
    "##### For date in hindi #####\n",
    "os.system(\"\"\"taskkill /f /im excel.exe\"\"\")\n",
    "yesterday=datetime.today()-timedelta(days=1)\n",
    "hindi_number_list=[]\n",
    "for i in range(2406,2416):\n",
    "    hindi_number_list.append(chr(i))\n",
    "\n",
    "english_number_list=[]\n",
    "for i in range(48,58):\n",
    "    english_number_list.append(chr(i))\n",
    "    \n",
    "months_in_hindi=[\"जनवरी\",\"फरवरी\",\" मार्च \",\"अप्रैल\",\"मई\",\"जून\",\"जुलाई\",\"अगस्त\",\"सितम्बर\",\"अक्टूबर\",\"नवम्बर\",\"दिसम्बर\"]\n",
    "months_in_english=[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "\n",
    "\n",
    "part_1=[]\n",
    "for i in yesterday.strftime(\"%d %B %Y\").split(\" \")[0]:\n",
    "        part_1.append(hindi_number_list[int(i)])\n",
    "part_1=''.join(part_1)\n",
    "\n",
    "part_2= months_in_hindi[months_in_english.index(yesterday.strftime(\"%d %B %Y\").split(\" \")[1])]\n",
    "\n",
    "part_3=[]\n",
    "for i in yesterday.strftime(\"%d %B %Y\").split(\" \")[2]:\n",
    "        part_3.append(hindi_number_list[int(i)])\n",
    "part_3=''.join(part_3)\n",
    "\n",
    "hindi_date= part_1+\" \"+part_2+\" \"+part_3\n",
    "\n",
    "#####################\n",
    "\n",
    "os.system(\"\"\"taskkill /f /im excel.exe\"\"\")\n",
    "if mismatch_track==6:\n",
    "    if (run_only_if_all_files_are_present==1) and (len(duplicacy_check) == 0):\n",
    "        sleep(8)\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Varanasi_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_1}\")\n",
    "        except (FileNotFoundError,PermissionError):\n",
    "            pass\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Bhubaneshwar_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_2}\")\n",
    "        except (FileNotFoundError,PermissionError):    \n",
    "            pass\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Patna_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_3}\")\n",
    "        except (FileNotFoundError,PermissionError):\n",
    "            pass\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Ranchi_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_4}\")\n",
    "        except (FileNotFoundError,PermissionError):\n",
    "            pass\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Jamshedpur_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_5}\")\n",
    "        except (FileNotFoundError,PermissionError):\n",
    "            pass\n",
    "        try:\n",
    "            os.rename(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\Cuttack_DPR.xlsm\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\{remember_me_6}\")\n",
    "        except (FileNotFoundError,PermissionError):\n",
    "            pass\n",
    "        for process in psutil.process_iter():\n",
    "            if \"excel\" in process.name().lower() :\n",
    "                process.kill()\n",
    "        yesterday=datetime.today()-timedelta(days=1)\n",
    "        try:\n",
    "            os.chdir(os.getcwd()+'\\\\COMBINED_DPR')\n",
    "        except FileNotFoundError:\n",
    "            os.chdir(os.getcwd())\n",
    "            \n",
    "        comb_dpr_files= os.listdir()\n",
    "\n",
    "        msg= EmailMessage()\n",
    "        sender= 'gailcgdwork@gmail.com'\n",
    "        \n",
    "        msg['From']=sender\n",
    "        msg['To']=\", \".join(recipients)\n",
    "        msg['CC']=\", \".join(cc)\n",
    "        msg['Subject']= f\"संयुक्त दैनिक प्रगति विवरण - दिनांक '{hindi_date}' तक की प्रगति / Combined DPR - Progress till {yesterday.strftime('%d-%b-%y')} | 6 Cities CGD Project\"\n",
    "        msg.set_content(\n",
    "\n",
    "        f\"\"\"आदरणीय महोदय/Dear Sir,\n",
    "\n",
    "कृपया छह शहरों में परियोजना संबंधी निर्माण स्थलों से प्राप्त दैनिक प्रगति विवरण के आधार पर दिनांक {hindi_date}  तक का संयुक्त दैनिक प्रगति विवरण प्राप्त करें। विभिन्न चरों की तुलना करने वाली पट्टी तालिका नीचे प्रस्तुत है:\n",
    "Please find attached, the combined DPR with Progress till {yesterday.strftime(\"%d %B %Y\")}, prepared, on the basis of site DPRs received. Bar charts comparing different variables are produced below: \n",
    "\n",
    "\n",
    "\n",
    "उपरोक्त दैनिक प्रगति विवरण का संदर्भ-चित्र नीचे प्रस्तुत है:\n",
    "Snapshot of the above mentioned combined DPR is captured below for ready reference:\n",
    "\n",
    "\n",
    "\n",
    "संयुक्त दैनिक प्रगति विवरण (दै.प्र.वि.) - संपीडित प्राकृतिक गैस स्टेशन:\n",
    "Combined Daily Progress Report (DPR) - CNG stations:\n",
    "\n",
    "\n",
    "\n",
    "यह अधोहस्ताक्षरी द्वारा स्वतः उत्पन्न विपत्र है:\n",
    "THIS IS AN AUTOGENERATED EMAIL, ON BEHALF OF THE BELOW SIGNED:\n",
    "\n",
    "\n",
    "सादर अभिवादन / Regards,\n",
    "निशांत सिन्हा / Nishant Sinha\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        )\n",
    "\n",
    "        for i in comb_dpr_files:\n",
    "            try:\n",
    "                with open(i, 'rb') as f:\n",
    "                    file_data= f.read()\n",
    "                    file_name= f.name\n",
    "                    f.close()\n",
    "            except PermissionError:\n",
    "                continue\n",
    "                \n",
    "            if \"Construction\".lower() not in file_name.lower():\n",
    "                if \".htm\".lower() not in file_name.lower():\n",
    "                    msg.add_attachment(file_data,maintype='application',subtype='octet-stream',filename=file_name)\n",
    "                \n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com',465) as smtp:\n",
    "            smtp.login('gailcgdwork@gmail.com', '<YOUR PASSWORD HERE>')\n",
    "            smtp.send_message(msg)\n",
    "\n",
    "#         %cd C:\\Users\\HM6176\\hp\\Documents\\Python Scripts\n",
    "            \n",
    "            ## create a copy of the databank last 7 days ##\n",
    "            \n",
    "        timestr= strftime(\"%d%m%Y\")\n",
    "#         shutil.copyfile(f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\CGD_DPR_DATA_BANK.xlsm\",f\"C:\\\\Users\\\\hp\\\\Pictures\\\\databank_backup_last_7_days\\\\CGD_DPR_DATA_BANK_{timestr}.xlsm\")\n",
    "        shutil.make_archive(f\"C:\\\\Users\\\\hp\\\\Pictures\\\\databank_backup_last_7_days\\\\DPR_BACKUP_STATE_{timestr}\",\"zip\",f\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\\\\\")\n",
    "        \n",
    "        %cd C:\\Users\\HM6176\\hp\\Documents\\Python Scripts\\COMBINED_DPR\n",
    "        \n",
    "        os.startfile(f\"{os.getcwd()}\\\\{[i for i in comb_dpr_files if '.xlsx' in i][0]} \")\n",
    "        os.startfile(f\"{os.getcwd()}\\\\{[i for i in comb_dpr_files if '.htm' in i][0]} \")\n",
    "        \n",
    "        for i in os.listdir(\"C:\\\\Users\\\\hp\\\\Pictures\\\\databank_backup_last_7_days\"):\n",
    "            if time()-os.path.getmtime(f\"C:\\\\Users\\\\hp\\\\Pictures\\\\databank_backup_last_7_days\\\\{i}\") > 6*24*60*60:\n",
    "                os.remove(f\"C:\\\\Users\\\\hp\\\\Pictures\\\\databank_backup_last_7_days\\\\{i}\")\n",
    "        \n",
    "    else:\n",
    "        print('No mail shall be sent. Not all files have been received.')\n",
    "else:\n",
    "    print(\"There seems to be some mismatch in the Oversimplified and Simplified file. Please check for mistakes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending files for reports every 15th and last day of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mismatch_track==6:\n",
    "    if (run_only_if_all_files_are_present==1) and (len(duplicacy_check) == 0):\n",
    "        os.chdir(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")\n",
    "        if datetime.today().date().day in [16,1]:\n",
    "            files_for_backup= ['BHUBANESHWAR','CUTTACK','JAMSHEDPUR','PATNA','RANCHI','VARANASI','CGD_DPR_DATA_BANK']\n",
    "            msg= EmailMessage()\n",
    "            sender= 'gailcgdwork@gmail.com'\n",
    "            msg['From']=sender\n",
    "            msg['To']= \", \".join(send_for_reporting)\n",
    "            msg['Subject']= f'Autogenerated mail: Data Bank updated on: {datetime.now().strftime(\"%d/%m/%y %H:%M:%S\")} with site DPRs'\n",
    "            for i in os.listdir():\n",
    "                for j in files_for_backup:\n",
    "                    if j.lower() in i.lower():\n",
    "                        if '.xlsm' in i or '.xlsb' in i:\n",
    "                            with open(i, 'rb') as f:\n",
    "                                file_data= f.read()\n",
    "                                file_name= f.name \n",
    "                                f.close()\n",
    "                            msg.add_attachment(file_data,maintype='application',subtype='octet-stream',filename=file_name)\n",
    "\n",
    "            with smtplib.SMTP_SSL('smtp.gmail.com',465) as smtp:\n",
    "                smtp.login('gailcgdwork@gmail.com', '<YOUR PASSWORD HERE>')\n",
    "                smtp.send_message(msg)\n",
    "            os.chdir('C:\\\\Users\\\\hp\\\\Documents')\n",
    "            print(\"Message sent successfully\")\n",
    " \n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\hp\\\\Documents\\\\Python Scripts\")\n",
    "try:\n",
    "    \n",
    "    for i in os.listdir():\n",
    "        if '.xlsx' in i:\n",
    "            os.remove(i)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mismatch_track==6:\n",
    "    if (run_only_if_all_files_are_present==1) and (len(duplicacy_check) == 0):\n",
    "        root=Tk()\n",
    "        tkinter.messagebox.showinfo('Do Not Forget to: ','Send the Construction DPR on WhatsApp')\n",
    "        subprocess.Popen([\"cmd\", \"/C\", \"start whatsapp://send?phone=+919990046918\"], shell=True)\n",
    "#         root.mainloop()\n",
    "        root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New checks applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "    df_main = pd.read_excel(\"CGD_DPR_DATA_BANK.xlsb\",engine='pyxlsb',sheet_name=\"Data\")\n",
    "    df_main.columns=df_main.iloc[0]\n",
    "    df_main.drop(index=0,inplace=True)\n",
    "else:\n",
    "    print(\"Not all files have been received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 of 4 ---> checks for steel welding, steel laying and steel charging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Varanasi \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Ranchi \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Cuttack \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Welding data for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Steel Laying data and Charging data for Patna \u001b[39m \u001b[0m\n",
      "\n",
      " ******************************************************************\n"
     ]
    }
   ],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "\n",
    "    check_flag_1= 0\n",
    "    warning_in_part_1=0\n",
    "    for i in df_main['CITY'].unique():\n",
    "        st_lay_sum=df_main[(df_main[\"ACTIVITY\"]=='Steel Laying') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        st_wel_sum=df_main[(df_main[\"ACTIVITY\"]=='Welding') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        st_char_sum=df_main[(df_main[\"ACTIVITY\"]=='Charged') & (df_main[\"CITY\"]==i) & (df_main[\"Project\"]==\"Steel Network\")]['PROGRESS Mtr'].sum()\n",
    "\n",
    "        if st_lay_sum>st_wel_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Steel Laying is {round(st_lay_sum-st_wel_sum,2)} M more than Steel Welding for {i} {Fore.RESET} {Style.RESET_ALL}\")            \n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in Steel Laying data and Welding data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_1+=1\n",
    "        if st_char_sum>st_lay_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Steel Charging is {round(st_char_sum-st_lay_sum,2)} M more than Steel Laying for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in Steel Laying data and Charging data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_1+=1\n",
    "        print(\"\\n ******************************************************************\")\n",
    "    if check_flag_1 < 12:\n",
    "        warning_in_part_1=1\n",
    "else:\n",
    "    print(\"Not all files have been received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 of 4 ---> For MDPE Laying vs Charging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in MDPE Laying data and Charging data for Patna \u001b[39m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "\n",
    "    check_flag_2=0\n",
    "    warning_in_part_2=0\n",
    "    for i in df_main['CITY'].unique():\n",
    "        pe_lay_sum=df_main[(df_main[\"ACTIVITY\"]=='PE Laying') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        pe_char_sum=df_main[(df_main[\"ACTIVITY\"]=='Charged') & (df_main[\"CITY\"]==i) & (df_main[\"Project\"]=='MDPE Network')]['PROGRESS Mtr'].sum()    \n",
    "        if st_char_sum>st_lay_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: MDPE Charging is {round(pe_char_sum-pe_lay_sum,2)} M more than Steel Laying for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in MDPE Laying data and Charging data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_2+=1\n",
    "\n",
    "    if check_flag_2 < 6:\n",
    "        warning_in_part_2=1\n",
    "\n",
    "else:\n",
    "    print(\"Not all files have been received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 of 4 ---> For LMC Riser, Meter and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Varanasi \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Ranchi \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Jamshedpur \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Cuttack \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Meter data for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Meter and Conversion data for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1m No issue found in Riser and Conversion data for Patna \u001b[39m \u001b[0m\n",
      "\n",
      "************************************************\n"
     ]
    }
   ],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "\n",
    "\n",
    "\n",
    "    check_flag_3=0\n",
    "    warning_in_part_3=0\n",
    "\n",
    "    for i in df_main['CITY'].unique():\n",
    "        riser_sum=df_main[(df_main[\"ACTIVITY\"]=='Connection with Riser') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        meter_sum=df_main[(df_main[\"ACTIVITY\"]=='Connection with Meter') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum() \n",
    "        conversion_sum=df_main[(df_main[\"ACTIVITY\"]=='PNG Conversion') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum() \n",
    "\n",
    "        if riser_sum < meter_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Number of Meter is {round(meter_sum-riser_sum,2)} more for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in Riser and Meter data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_3+=1\n",
    "        if meter_sum < conversion_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Number of Conversion is {round(conversion_sum-meter_sum,2)} more for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in Meter and Conversion data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_3+=1\n",
    "        if riser_sum < conversion_sum:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Please Note: Number of Meter is {round(conversion_sum-riser_sum,2)} more for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT} No issue found in Riser and Conversion data for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_3+=1\n",
    "\n",
    "        print(\"\\n************************************************\")\n",
    "    if check_flag_3 < 18:\n",
    "        warning_in_part_3=1   \n",
    "        \n",
    "else:\n",
    "    print(\"Not all files have been received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 of 4 ---> Abnormal Value Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*****************  For Varanasi: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Varanasi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Varanasi \u001b[39m \u001b[0m\n",
      "\n",
      "\n",
      "*****************  For Bhubaneshwar: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Bhubaneshwar \u001b[39m \u001b[0m\n",
      "\n",
      "\n",
      "*****************  For Ranchi: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Ranchi \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Ranchi \u001b[39m \u001b[0m\n",
      "\n",
      "\n",
      "*****************  For Jamshedpur: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Jamshedpur \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Jamshedpur \u001b[39m \u001b[0m\n",
      "\n",
      "\n",
      "*****************  For Cuttack: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Cuttack \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Cuttack \u001b[39m \u001b[0m\n",
      "\n",
      "\n",
      "*****************  For Patna: ----------->\n",
      "\u001b[32m \u001b[1mReported riser figure is below 200 for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported meter figure is below 200 for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported conversion figure is below 200 for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported PE laying figure is below 1500 M for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported welding figure is below 600 M for Patna \u001b[39m \u001b[0m\n",
      "\u001b[32m \u001b[1mReported steel laying figure is below 600 M for Patna \u001b[39m \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "\n",
    "\n",
    "    check_flag_4=0\n",
    "    warning_in_part_4=0\n",
    "\n",
    "    aaj_ke_dpr_ka_progress=df_main[df_main[\"DATE\"]== df_main[\"DATE\"].tail(10000).unique().max()]\n",
    "    riser_flag=meter_flag=conversion_flag=200\n",
    "    pe_lay_flag= 1500\n",
    "    welding_flag= 600\n",
    "    steel_lay_flag= 600\n",
    "    for i in df_main['CITY'].unique():\n",
    "        print(\"\\n\")\n",
    "        print(f\"*****************  For {i}: ----------->\")\n",
    "        riser_sum_today=aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='Connection with Riser') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        meter_sum_today=aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='Connection with Meter') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        pe_lay_sum_today=aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='PE Laying') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum() \n",
    "        png_conv_today= aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='PNG Conversion') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        welding_today= aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='Welding') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "        steel_laying_today= aaj_ke_dpr_ka_progress[(aaj_ke_dpr_ka_progress[\"ACTIVITY\"]=='Steel Laying') & (df_main[\"CITY\"]==i) ]['PROGRESS Mtr'].sum()\n",
    "\n",
    "        if riser_sum_today > riser_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value ({riser_sum_today}) for Riser in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported riser figure is below {riser_flag} for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "        if meter_sum_today > meter_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value ({meter_sum_today}) for Meter in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported meter figure is below {meter_flag} for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "\n",
    "        if png_conv_today > conversion_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value ({png_conv_today}) for Domestic Conversion in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported conversion figure is below {conversion_flag} for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "\n",
    "        if pe_lay_sum_today > pe_lay_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value ({pe_lay_sum_today}) M for PE Laying in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported PE laying figure is below {pe_lay_flag} M for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "\n",
    "        if welding_today > welding_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value for Welding ({welding_today}) M in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported welding figure is below {welding_flag} M for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "        if steel_laying_today > steel_lay_flag:\n",
    "            print(f\"{Fore.YELLOW} {Style.BRIGHT} Abnormally high value for Steel Laying ({steel_laying_today}) in {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "        else:\n",
    "            print(f\"{Fore.GREEN} {Style.BRIGHT}Reported steel laying figure is below {steel_lay_flag} M for {i} {Fore.RESET} {Style.RESET_ALL}\")\n",
    "            check_flag_4+=1\n",
    "\n",
    "    if check_flag_4 < 36:\n",
    "        warning_in_part_4=1     \n",
    "        \n",
    "else:\n",
    "    print(\"Not all files have been received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR SPEECH INITIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_speech(part):\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    text = f\"Have a look, it seems there is a warning in part {part}\"\n",
    "    engine.setProperty('voice', 'english_rp+f3')\n",
    "    engine.setProperty('rate',160)\n",
    "    engine.say(text)\n",
    "\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mismatch_track==6 and (run_only_if_all_files_are_present==1 and (len(duplicacy_check) == 0))):\n",
    "\n",
    "\n",
    "    def show_warning(part):\n",
    "        root = Tk()\n",
    "        tkinter.messagebox.showinfo(title=f\"Warning Message (Part {part})\", message=f\"There is a warning in Part {part}. Please check\")\n",
    "        warning_speech(part)\n",
    "        root.destroy()\n",
    "\n",
    "    warnings = 0\n",
    "    for part in range(1, 5):\n",
    "        if locals()[f\"warning_in_part_{part}\"]:\n",
    "            show_warning(part)\n",
    "            warnings += 1\n",
    "\n",
    "    if warnings == 0:\n",
    "        root = Tk()\n",
    "        tkinter.messagebox.showinfo(title=\"No Warning :) \", message=\"There     is     no     Warning. \\n\\n Mail Aage Bhej Do Nishant.\")\n",
    "        root.destroy()\n",
    "        \n",
    "else:\n",
    "    print(\"Not all files have been received\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of days: 525 since first implementation of tracking on 24th July 2022: \n",
      "\n",
      "GA wise delay count is as below: \n",
      "\n",
      "varanasi        70\n",
      "bhubaneshwar    21\n",
      "patna           90\n",
      "ranchi          83\n",
      "jamshedpur      16\n",
      "cuttack         58\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Number of days: {dpr_delay_count.day.count()} since first implementation of tracking on 24th July 2022: \\n\")\n",
    "print(\"GA wise delay count is as below: \\n\")\n",
    "print(dpr_delay_count.iloc[:,1:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder count for the day is as below:\n",
      " \n",
      "   varanasi  bhubaneshwar  patna  ranchi  jamshedpur  cuttack\n",
      "0         0             0      0       0           0        0\n"
     ]
    }
   ],
   "source": [
    "print(\"Reminder count for the day is as below:\\n \")\n",
    "print(dpr_reminder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "कल की प्रगति दर्शाने वाली दैनिक प्रगति विवरण इस विपत्र के प्रेषण तक रांची से प्राप्त नहीं हुई है। अतः संयुक्त दैनिक प्रगति प्रतिवेदन में रांची की प्रगति को शून्य के रूप में अद्यतन किया गया है।\n",
    "\n",
    "Daily progress report, showing yesterday's progress has not been received from Ranchi till the time of this email. Hence, the progress for Ranchi has been updated as zero in combined daily progress report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code generation for Macros [SITE DPR]\n",
    "\n",
    "### For addition of new lines, to clearing the \"today\" content in the Steel Sheet, run the below code and enter the column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= \"\"\"Sheets(\"01.Steel Laying\").Range(\"\"\"\n",
    "# b= '\"F10:F13,F15:F18,F20:F23,F25:F28,F30:F33,F35:F38,F39:F45\"'\n",
    "# c= \").ClearContents\"\n",
    "# old_col=\"F\"\n",
    "# new_column= 1\n",
    "# while new_column!= \"n\":\n",
    "#     new_col_name= input(\"Enter the Column name: \")\n",
    "#     b= b.replace(old_col,new_col_name)\n",
    "#     old_col=new_col_name\n",
    "#     print(\"Paste the code below in macros: \")\n",
    "#     print('\\n')\n",
    "#     print(a+b+c)\n",
    "#     print('\\n')\n",
    "#     new_column=input(\"another column? Press 'n' to exit, enter to continue: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: for the MDPE part, the code is simple and may be copied from Macros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For addition of lines, to update the previous value in the Steel Sheet, run the below code and enter the columns. Copy the output and paste in the macros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string1= \"\"\"Sheets(\"01.Steel Laying\").Range(\"\"\"\n",
    "# list2=[('\"G10: G13\"'),('\"E10: E13\"'),('\"G15: G18\"'),('\"E15: E18\"'),('\"G20: G23\"'),('\"E20: E23\"'),\n",
    "#        ('\"G25: G28\"'),('\"E25: E28\"'),('\"G30: G33\"'),('\"E30: E33\"'),('\"G35: G38\"'),('\"E35: E38\"'),('\"G39: G45\"'),\n",
    "#        ('\"E39: E45\"')]\n",
    "# string3=\").Copy\"\n",
    "# string4=\").PasteSpecial xlPasteValues\"\n",
    "\n",
    "# new_column= 1\n",
    "# while new_column!= \"n\":\n",
    "\n",
    "#     cumulative_column= input(\"Enter the Cumulative Column: \")\n",
    "#     previous_column = input(\"Enter the Previous Column: \")\n",
    "#     print(\"Copy and paste the code below: \\n\")\n",
    "\n",
    "#     for i,j in enumerate(list2):\n",
    "#         if(int(i)%2==0):\n",
    "#             print(string1+j.replace(\"G\",cumulative_column)+string3)\n",
    "#         elif(int(i)%2==1):\n",
    "#             print(string1+j.replace(\"E\",previous_column)+string4)\n",
    "#     print('\\n')\n",
    "#     new_column=input(\"another column? Press 'n' to exit, enter to continue: \")\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: for the MDPE part, the code is simple and may be copied from Macros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Additional Notes: Uncomment the above two cells and run, in case of addition of new vendor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
